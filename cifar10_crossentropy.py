# -*- coding: utf-8 -*-
"""CIFAR10 CrossEntropy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KF8SsmambKcktysUvZOqvjNkD_d1IM4t
"""

#Import All Required Libraries

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

#Switch to gpu
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

#Load MINST data
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
train_dataset = torchvision.datasets.CIFAR10(root="./data", train=True, transform=transform, download=True)
test_dataset = torchvision.datasets.CIFAR10(root="./data", train=False, transform=transform, download=True)

#Create Dataloaders to feed data to models in batches
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True)

classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv_stack = nn.Sequential(

            # Find Important Features
            #(Input: 3 channels (RGB), Output: 32 channels)
            nn.Conv2d(3, 32, kernel_size=3, padding=1),

            #Non-Linear
            nn.ReLU(),

            #Shrink & Refine
            nn.MaxPool2d(kernel_size=2, stride=2),

            #Find Important Features Again
            nn.Conv2d(32, 64, kernel_size=3, padding=1),

            #Non-Linear
            nn.ReLU(),

            #Shrink & Refine Again
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.fc_stack = nn.Sequential(

            #64 channel map, 8 x 8 size, 128 Neurons
            nn.Linear(64 * 8 * 8, 128),  # Fully Connected Layer 1

            #Non-Linear
            nn.ReLU(),

            #Helps Prevent Overfitting
            nn.Dropout(0.2),

            #Final Classification
            nn.Linear(128, 10)  # Output layer (10 classes)
        )

    #How the model goes forward
    def forward(self, x):

        #First through Convolutional Layers
        x = self.conv_stack(x)

        #Need to flatten because feature maps need a single vector
        #Also creates a vector of the size that we need
        x = x.view(x.size(0), -1)

        #Run i through the fully connected layers
        x = self.fc_stack(x)
        return x

# Initialize model
model = CNN().to(device)

#Cross entropy loss, not MSE
criterion = nn.CrossEntropyLoss()

#Gradient Descent, but can also use Adam
optimizer = optim.Adam(model.parameters(), lr=0.0005)

#Train the model
epochs = 15

for epoch in range(epochs):
    #Initalize
    correct, total, stat_loss = 0, 0, 0

    #Set the Model To Train Mode
    model.train()

    for images, labels in train_loader:

      images = images.to(device)
      labels = labels.to(device)

      #Resets gradiant from prev loop
      optimizer.zero_grad()

      #Data is fed into the model
      outputs = model(images)

      #Figure out how wrong it is
      loss = criterion(outputs, labels)

      #Adjust according to the loss
      loss.backward()

      #Update the optimizer
      optimizer.step()

      #Accumulate loss for each epoch
      stat_loss += loss.item()

      #Gets predicted class for each image
      _, predicted = torch.max(outputs, 1)

      #Updates no. of images seen so far
      total += labels.size(0)

      #Updates no. of correct predictions
      correct += (predicted == labels).sum().item()

    #Calculate the avg. loss for each epoch
    stat_loss = stat_loss / len(train_loader)

        # Calculate accuracy as a percentage
    accuracy = 100 * correct / total

    #Evaluate on test set
    model.eval()
    test_correct, test_total, test_loss = 0, 0, 0

    with torch.no_grad():  # Disable gradient calculation during evaluation
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            test_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            test_total += labels.size(0)
            test_correct += (predicted == labels).sum().item()

    # Calculate average test loss
    test_loss = test_loss / len(test_loader)

    # Calculate test accuracy
    test_accuracy = 100 * test_correct / test_total

    # Print training and test metrics for the epoch
    print(f"Epoch {epoch+1}/{epochs}, Training Loss: {stat_loss:.4f}, Training Accuracy: {accuracy:.2f}%")
    print(f"Epoch {epoch+1}/{epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\n")

#Visualize some images with their predictions
fig, axes = plt.subplots(3, 3, figsize=(6, 6))

#Tell the model to stop learning
model.eval()

with torch.no_grad():
  #Get batch of images
  images, labels = next(iter(test_loader))

  images = images.to(device)
  labels = labels.to(device)

  #Predictions
  outputs = model(images) #_flattened)
  _, predictions = torch.max(outputs, 1)

#DISPLAY *
  for i, ax in enumerate(axes.flat):

      #***Transpose the image to move the channel dimension to the last position
      #***permute rearranges the dimensions: (Channel,Height,Width) -> (Height,Width,Channel)
      img_transposed = images[i].cpu().permute(1, 2, 0)

      #Get the predicted class name using the index
      predicted_class_name = classes[predictions[i].item()]

      ax.imshow(img_transposed)
      ax.set_title(f"Prediction: {predicted_class_name}")
      ax.axis("off")

plt.show()